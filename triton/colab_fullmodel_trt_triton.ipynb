{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SLEAP_TRT_Triton",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wolfffff/gpuhackathon-sleap/blob/main/colab_fullmodel_trt_triton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2-3Ho5R8hqH"
      },
      "source": [
        "# A Simple Triton Example with a SLEAP Model\n",
        "\n",
        "For this to work, you'll need to have a working Triton Inference Server serving your model of interest. In this case, we're using a one exposed through [ngrok](https://ngrok.com/). We can simply pull our data of interest and send it directly to our inference server using gRPC -- or HTTP if we wanted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU44U3ZZ65HU"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!wget https://github.com/triton-inference-server/server/releases/download/v2.10.0/v2.10.0_ubuntu2004.clients.tar.gz\n",
        "!tar -zxvf v2.10.0_ubuntu2004.clients.tar.gz --wildcards python/tritonclient-2.10.0-py3-none-manylinux1_x86_64.whl --strip-components 1\n",
        "!wget https://raw.githubusercontent.com/Wolfffff/gpuhackathon-sleap/main/triton/triton_utils.py\n",
        "!wget -P data https://storage.googleapis.com/sleap-data/reference/flies13/190719_090330_wt_18159206_rig1.2%4015000-17560.mp4\n",
        "!wget -P data https://storage.googleapis.com/sleap-data/reference/flies13/190719_090330_wt_18159206_rig1.2%4015000-17560.slp"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ0GX-RC78UR"
      },
      "source": [
        "%%capture\n",
        "!pip install sleap\n",
        "# For some reason triton client install only works from their .whl, so we pull the release, extract, and install.\n",
        "!pip install tritonclient-2.10.0-py3-none-manylinux1_x86_64.whl[all]\n",
        "\n",
        "# Restart to deal with package import issue...\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtAX9MTO9DW2"
      },
      "source": [
        "import sleap\n",
        "import tritonclient.grpc as grpcclient\n",
        "import palettable\n",
        "import cv2\n",
        "import time\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzDu2bR7xl8e"
      },
      "source": [
        "slp_path = \"data/190719_090330_wt_18159206_rig1.2@15000-17560.slp\"\n",
        "slp_labels = sleap.load_file(slp_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbidrDTH8OTz"
      },
      "source": [
        "def read_frames(video_path, fidxs=None, grayscale=True):\n",
        "    \"\"\"Read frames from a video file.\n",
        "    \n",
        "    Args:\n",
        "        video_path: Path to MP4\n",
        "        fidxs: List of frame indices or None to read all frames (default: None)\n",
        "        grayscale: Keep only one channel of the images (default: True)\n",
        "    \n",
        "    Returns:\n",
        "        Loaded images in array of shape (n_frames, height, width, channels) and dtype uint8.\n",
        "    \"\"\"\n",
        "    vr = cv2.VideoCapture(video_path)\n",
        "    if fidxs is None:\n",
        "        fidxs = np.arange(vr.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frames = []\n",
        "    for fidx in fidxs:\n",
        "        vr.set(cv2.CAP_PROP_POS_FRAMES, fidx)\n",
        "        img = vr.read()[1]\n",
        "        if grayscale:\n",
        "            img = img[:, :, [0]]\n",
        "        frames.append(img)\n",
        "    return np.stack(frames, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnpZZUBS8s1x"
      },
      "source": [
        "# Exposed localhost though ngrok -- this is the main retricting factor here.\n",
        "# ngrok seems to be exceptionally slow but probably fine for this example...\n",
        "video_path = \"data/190719_090330_wt_18159206_rig1.2@15000-17560.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "\n",
        "triton_url = '2.tcp.ngrok.io:19256'\n",
        "model_name = \"ex\"\n",
        "model_version=\"1\"\n",
        "protocol = 'grpc'\n",
        "triton_client = grpcclient.InferenceServerClient(url=triton_url)\n",
        "sent_count = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS8SCAsO-AAW"
      },
      "source": [
        "# Built off of \n",
        "# https://github.com/triton-inference-server/client/blob/main/src/python/examples/image_client.py\n",
        "import argparse\n",
        "from functools import partial\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tritonclient.grpc as grpcclient\n",
        "import tritonclient.http as httpclient\n",
        "\n",
        "\n",
        "def parse_model(model_metadata, model_config):\n",
        "  \"\"\"\n",
        "    Check model definition get names \n",
        "  \"\"\"\n",
        "  if len(model_metadata.inputs) != 1:\n",
        "      raise Exception(\"expecting 1 input, got {}\".format(\n",
        "          len(model_metadata.inputs)))\n",
        "  if len(model_metadata.outputs) != 4:\n",
        "      raise Exception(\"expecting 4 outputs, got {}\".format(\n",
        "          len(model_metadata.outputs)))\n",
        "\n",
        "  if len(model_config.input) != 1:\n",
        "      raise Exception(\n",
        "          \"expecting 1 input in model configuration, got {}\".format(\n",
        "              len(model_config.input)))\n",
        "\n",
        "  input_metadata = model_metadata.inputs[0]\n",
        "  if input_metadata.datatype != \"UINT8\":\n",
        "      raise Exception(\"expecting output datatype to be UINT8, model '\" +\n",
        "                      model_metadata.name + \"' output type is \" +\n",
        "                      input_metadata.datatype)\n",
        "  input_config = model_config.input[0]\n",
        "  output_metadata = model_metadata.outputs[0]\n",
        "\n",
        "\n",
        "  output_batch_dim = (model_config.max_batch_size > 0)\n",
        "  non_one_cnt = 0\n",
        "  for dim in output_metadata.shape:\n",
        "      if output_batch_dim:\n",
        "          output_batch_dim = False\n",
        "      elif dim > 1:\n",
        "          non_one_cnt += 1\n",
        "          if non_one_cnt > 100:\n",
        "              raise Exception(\"expecting model output to be a vector\")\n",
        "\n",
        "  # Should be [-1,1024,1024,1] but not batching -- they're processed as a single input.\n",
        "  expected_input_dims = 4 \n",
        "  if len(input_metadata.shape) != expected_input_dims:\n",
        "      raise Exception(\n",
        "          \"expecting input to have {} dimensions, model '{}' input has {}\".\n",
        "          format(expected_input_dims, model_metadata.name,\n",
        "                  len(input_metadata.shape)))\n",
        "\n",
        "  n = input_metadata.shape[0]\n",
        "  h = input_metadata.shape[1]\n",
        "  w = input_metadata.shape[2]\n",
        "  c = input_metadata.shape[3]\n",
        "\n",
        "  return (model_config.max_batch_size, input_metadata.name,\n",
        "          output_metadata.name, c, h, w, input_config.format,\n",
        "          input_metadata.datatype)\n",
        "\n",
        "def requestGenerator(batched_image_data, input_name, output_names, dtype, protocol,model_name,model_version):\n",
        "  \n",
        "  if protocol == \"grpc\":\n",
        "      client = grpcclient\n",
        "  else:\n",
        "      client = httpclient\n",
        "\n",
        "  # Set the input data\n",
        "  inputs = [client.InferInput(input_name, batched_image_data.shape, dtype)]\n",
        "  inputs[0].set_data_from_numpy(batched_image_data)\n",
        "\n",
        "\n",
        "  outputs = []\n",
        "  for name in output_names:\n",
        "      outputs.append(client.InferRequestedOutput(name))\n",
        "\n",
        "  yield inputs, outputs, model_name, model_version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2syOiN67EYP"
      },
      "source": [
        "model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version)\n",
        "\n",
        "model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)\n",
        "\n",
        "model_config = model_config.config\n",
        "\n",
        "# Base model info\n",
        "max_batch_size, input_name, output_name, c, h, w, format, dtype = parse_model(model_metadata, model_config)\n",
        "\n",
        "# Fix output names for when we have multiple\n",
        "output_names = [model.name for model in model_metadata.outputs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96i_wSgS-cQJ"
      },
      "source": [
        "Pair all of our model info, generate request, and send it off!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSN8ct3V86pr"
      },
      "source": [
        "# Pair request generator \n",
        "def query_triton(frame):\n",
        "  responses = []\n",
        "\n",
        "  # Bad practice... but it saves some time\n",
        "  global model_name, model_version, sent_count, protocol, sent_count\n",
        "  \n",
        "  for inputs, outputs,model_name, model_version in requestGenerator(\n",
        "          frame, input_name, output_names, dtype, protocol,model_name,model_version):\n",
        "      responses.append(triton_client.infer(model_name,\n",
        "                                  inputs,\n",
        "                                  request_id=str(sent_count),\n",
        "                                  model_version=model_version,\n",
        "                                  outputs=outputs))\n",
        "      sent_count += 1\n",
        "                                  \n",
        "  return responses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TosqQw_-KPE"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from ipywidgets import interactive, fixed,widgets\n",
        "import time\n",
        "\n",
        "\n",
        "def plot_frame(video_path, fidx=0):\n",
        "\n",
        "  global h,w\n",
        "  start_timestamp = time.time()\n",
        "\n",
        "  # Fetch the image corresponding to the frame index\n",
        "  img = read_frames(video_path=video_path,fidxs=[fidx])\n",
        "\n",
        "  # Send to Triton server for inference!\n",
        "  start_timestamp_triton = time.time()\n",
        "  response = query_triton(img)\n",
        "  triton_inference_time = (time.time() - start_timestamp_triton)\n",
        "\n",
        "  points = response[0].as_numpy(output_names[0])\n",
        "\n",
        "  # Class probs to fix coloration\n",
        "  class_probabilities = response[0].as_numpy(output_names[2])\n",
        "  classes = np.argmax(class_probabilities,axis=1)\n",
        "\n",
        "  # Build sleap instances from response\n",
        "  instances = [sleap.Instance.from_numpy(points[i,:,:],skeleton=slp_labels.skeleton) for i in range(points.shape[0])]\n",
        "  sleap.nn.viz.plot_img(img,scale=0.5);\n",
        "  sleap.nn.viz.plot_instances(instances,cmap=np.array(palettable.wesanderson.Royal1_4.mpl_colors)[classes],alpha=0.5);\n",
        "  plt.show()\n",
        "\n",
        "  # # Log timing if you want!\n",
        "  # print (f'Processed frame: {fidx}')\n",
        "  # print ('\\nTriton request time: %.2f s.' % triton_inference_time)\n",
        "  # print ('\\nFrame processing time: %.2f s.' % (time.time() - start_timestamp))\n",
        "\n",
        "# Initialize and launch the widget making a fixed size so the text doesnt force scrolling\n",
        "max_frame_idx = int(frame_count) - 1\n",
        "interactive_plot = interactive(plot_frame, fidx=widgets.IntSlider(min=0, max=max_frame_idx, step=1, value=0,description='Frame ID'),video_path=fixed(video_path))\n",
        "# output = interactive_plot.children[-1]\n",
        "# output.layout.height = '1124px'\n",
        "interactive_plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yj_adFLHurv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}