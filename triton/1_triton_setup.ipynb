{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f471b8e0-91dc-47d3-a02d-cfe3cb45ef6c",
   "metadata": {},
   "source": [
    "### Simple Triton Setup for SavedModel\n",
    "\n",
    "See: https://github.com/triton-inference-server/server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc422337-20fa-4e9f-b167-ea3d657f1cdd",
   "metadata": {},
   "source": [
    "To start, make sure you are set up to use nvidia docker containers. See Talmo's description in [here](../tensorrt/installing_docker.md).\n",
    "\n",
    "\n",
    "Once we have that set up, we simply need to set up a model repository containing our models of choice -- in this case just using the SavedModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff58b1f5-e0f8-4f3c-9c94-b0f2fa63648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repository/centroid_savedmodel/1/model.savedmodel/\n",
    "!cp -r ../tensorrt/data/centroid_savedmodel/* model_repository/centroid_savedmodel/1/model.savedmodel/\n",
    "\n",
    "!mkdir -p model_repository/td_savedmodel/1/model.savedmodel/\n",
    "!cp -r ../tensorrt/data/td_savedmodel/* model_repository/td_savedmodel/1/model.savedmodel/\n",
    "\n",
    "!mkdir -p model_repository/bu_savedmodel/1/model.savedmodel/\n",
    "!cp -r ../tensorrt/data/bu_savedmodel/* model_repository/bu_savedmodel/1/model.savedmodel/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69433d-917c-436d-a2fe-6a63dc493a46",
   "metadata": {},
   "source": [
    "### You can define the count of GPUs -- 1 and 2 here respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259a3a62-eab5-4e9a-b5aa-1940486fc21e",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/wolf/git/gpuhackathon-sleap/triton/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models --backend-config=tensorflow,version=2\n",
    " \n",
    "docker run --gpus=2 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/wolf/git/gpuhackathon-sleap/triton/model_repository:/models nvcr.io/nvidia/tritonserver:21.04-py3 tritonserver --model-repository=/models --backend-config=tensorflow,version=2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e901b7-9af5-4ce2-b614-d934a9edbff5",
   "metadata": {},
   "source": [
    "docker run --gpus=2 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/wolf/git/gpuhackathon-sleap/triton/model_repository:/models nvcr.io/nvidia/tritonserver:21.04-py3 tritonserver --model-repository=/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b795c0c-7c49-4b73-ba9b-a6df1c5e9ea1",
   "metadata": {},
   "source": [
    "Now we can quickly make the config files defining our input and output. We definitely should think more about these configs if we go further with this (dynamic batching etc could prove useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a42ba5-c27a-4fda-8b42-f677127873ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp bu_config.pbtxt model_repository/bu_savedmodel/config.pbtxt\n",
    "!cp td_config.pbtxt model_repository/td_savedmodel/config.pbtxt\n",
    "!cp centroid_config.pbtxt model_repository/centroid_savedmodel/config.pbtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d88aee-b2fa-4aa2-9dd8-850e00d4cb12",
   "metadata": {},
   "source": [
    "# Prometheus\n",
    "\n",
    "We can also monitor this pretty easily using "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498bf12-d71c-42cd-9445-59a7bbe18a8a",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker run -p 9090:9090 -v {BASE}/gpuhackathon-sleap/triton/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
