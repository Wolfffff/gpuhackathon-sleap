{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc50b0cd-8ee2-4e50-8e94-e1a7e2265aa3",
   "metadata": {},
   "source": [
    "# Triton Benchmarking using perf_analyzer\n",
    "\n",
    "Straight from https://github.com/triton-inference-server/server/blob/main/docs/perf_analyzer.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6c03b-df00-4721-beef-62e6901fe967",
   "metadata": {},
   "source": [
    "!pip install tritonclient[all]Note the --gpus here\n",
    "```bash\n",
    "docker run --gpus=2 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/wolf/git/gpuhackathon-sleap/triton/model_repository:/models nvcr.io/nvidia/tritonserver:21.04-py3 tritonserver --model-repository=/models --metrics-port 8002\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb683b68-425f-4692-9679-d625dcfa1a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: tritonclient[all] in /root/miniconda3/envs/trt/lib/python3.6/site-packages (2.9.0)\n",
      "Requirement already satisfied: python-rapidjson>=0.9.1 in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from tritonclient[all]) (1.0)\n",
      "Requirement already satisfied: numpy>=1.19.1 in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from tritonclient[all]) (1.19.5)\n",
      "Requirement already satisfied: geventhttpclient>=1.4.4 in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from tritonclient[all]) (1.4.4)\n",
      "Requirement already satisfied: grpcio>=1.31.0 in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from tritonclient[all]) (1.34.1)\n",
      "Requirement already satisfied: protobuf>=3.5.0 in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from tritonclient[all]) (3.17.1)\n",
      "Requirement already satisfied: six in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (1.15.0)\n",
      "Requirement already satisfied: gevent>=0.13 in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (21.1.2)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from geventhttpclient>=1.4.4->tritonclient[all]) (2020.12.5)\n",
      "Requirement already satisfied: zope.interface in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (5.4.0)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (52.0.0.post20210125)\n",
      "Requirement already satisfied: greenlet<2.0,>=0.4.17 in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (1.1.0)\n",
      "Requirement already satisfied: zope.event in /root/miniconda3/envs/trt/lib/python3.6/site-packages (from gevent>=0.13->geventhttpclient>=1.4.4->tritonclient[all]) (4.5.0)\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libb64-dev is already the newest version (1.2-4).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libc6 is already the newest version (2.27-3ubuntu1.4).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 28 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install tritonclient[all]\n",
    "!apt install -y libb64-dev\n",
    "!apt install libc6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a3a4dbe-cf7e-4425-9678-b5a3a0abc11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perf_analyzer: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by perf_analyzer)\n",
      "perf_analyzer: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28' not found (required by perf_analyzer)\n"
     ]
    }
   ],
   "source": [
    "# Two GPUs\n",
    "\n",
    "!perf_analyzer -m td_savedmodel --concurrency-range 1:21:5 -i grpc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638662b8-869b-440e-90c8-54091d120993",
   "metadata": {},
   "source": [
    "Note the --gpus here\n",
    "```bash\n",
    "docker run --gpus=1 --rm -p8000:8000 -p8001:8001 -p8002:8002 -v/home/wolf/git/gpuhackathon-sleap/triton/model_repository:/models nvcr.io/nvidia/tritonserver:21.04-py3 tritonserver --model-repository=/models --metrics-port 8002\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166b018-e9a2-491b-bdd6-0ae6da6b86f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One GPU\n",
    "!perf_analyzer -m td_savedmodel --concurrency-range 1:21:5 -i grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2523491b-19af-4d05-9103-445f4b6f48c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Measurement Settings ***\n",
      "  Batch size: 32\n",
      "  Measurement window: 5000 msec\n",
      "  Using synchronous calls for inference\n",
      "  Stabilizing using average latency\n",
      "\n",
      "Request concurrency: 16\n",
      "  Client: \n",
      "    Request count: 53\n",
      "    Throughput: 339.2 infer/sec\n",
      "    Avg latency: 1286949 usec (standard deviation 37322 usec)\n",
      "    p50 latency: 1268248 usec\n",
      "    p90 latency: 1353941 usec\n",
      "    p95 latency: 1359903 usec\n",
      "    p99 latency: 1365078 usec\n",
      "    Avg gRPC time: 1279671 usec ((un)marshal request/response 2938 usec + response wait 1276733 usec)\n",
      "  Server: \n",
      "    Inference count: 2112\n",
      "    Execution count: 66\n",
      "    Successful request count: 66\n",
      "    Avg request latency: 1258670 usec (overhead 3248 usec + queue 1075959 usec + compute input 14760 usec + compute infer 163150 usec + compute output 1553 usec)\n",
      "\n",
      "Inferences/Second vs. Client Average Batch Latency\n",
      "Concurrency: 16, throughput: 339.2 infer/sec, latency 1286949 usec\n"
     ]
    }
   ],
   "source": [
    "# Two GPU\n",
    "!perf_analyzer -m centroid_savedmodel --concurrency-range 16 -b 32 -i grpc -f 2GPU_td_savedmodel_4instperGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6b1db-f103-4068-bf67-e44bbeaa79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two GPUs\n",
    "!perf_analyzer --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c31984-72bd-4216-9fe3-52606abb1b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
