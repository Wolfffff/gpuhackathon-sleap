{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5194ffd-0c8c-4783-9954-d442c0f2005e",
   "metadata": {},
   "source": [
    "# Convert models\n",
    "This notebook converts the HDF5-serialized raw SLEAP models, first into TF `SavedModel` format, and then into TensorRT-optimized format at different precisions.\n",
    "\n",
    "Before running this, see `download_data.ipynb`.\n",
    "\n",
    "This is based on the [TensorRT example notebook here](https://github.com/NVIDIA/TensorRT/blob/master/quickstart/IntroNotebooks/2.%20Using%20the%20Tensorflow%20TensorRT%20Integration.ipynb).\n",
    "\n",
    "See also: [TF-TRT integration](https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-722/quick-start-guide/index.html#framework-integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e88208-045d-4b35-81b5-400df112ea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"bu\", \"centroid\", \"td\"]\n",
    "# precisions = [\"FP32\", \"FP16\", \"INT8\"]  # INT8 is broken?\n",
    "precisions = [\"FP32\", \"FP16\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a521d82-0799-440f-b258-9815a5d5b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_savedmodel(model):\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "\n",
    "    h5_model_path = f\"data/{model}_sleap_model/best_model.h5\"\n",
    "    saved_model_path = f\"data/{model}_savedmodel\"  # SavedModel proto folder\n",
    "    if not os.path.exists(saved_model_path):\n",
    "        model = tf.keras.models.load_model(h5_model_path, compile=False)\n",
    "        model.save(saved_model_path)\n",
    "        print(f\"Saved: {h5_model_path} -> {saved_model_path}\")\n",
    "\n",
    "        \n",
    "def convert_to_trt(model, precision):\n",
    "    # https://github.com/NVIDIA/TensorRT/blob/master/quickstart/IntroNotebooks/2.%20Using%20the%20Tensorflow%20TensorRT%20Integration.ipynb\n",
    "    import os\n",
    "    \n",
    "    saved_model_path = f\"data/{model}_savedmodel\"  # SavedModel proto folder\n",
    "    opt_model_path = f\"data/{model}_trt_{precision}\"\n",
    "    \n",
    "    from trtutils import ModelOptimizer\n",
    "    opt_model = ModelOptimizer(saved_model_path)\n",
    "    print(f\"Created ModelOptimizer with: {saved_model_path}\")\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    if precision == \"INT8\":\n",
    "        import tensorflow as tf\n",
    "        tf_model = tf.keras.models.load_model(saved_model_path)\n",
    "        \n",
    "        # not working:\n",
    "        # InternalError:  Failed to feed calibration data\n",
    "        # [[node TRTEngineOp_2_0 (defined at /mnt/helper.py:94) ]] [Op:__inference_pruned_20725]\n",
    "\n",
    "        # Function call stack:\n",
    "        # pruned\n",
    "        N = 32\n",
    "        calib_data = np.zeros((N,) + tuple(tf_model.inputs[0].shape[1:]))\n",
    "        print(\"Set calibration data:\", calib_data.shape)\n",
    "        opt_model.set_calibration_data(calib_data)\n",
    "    \n",
    "    opt_model_ = opt_model.convert(opt_model_path, precision=precision)\n",
    "\n",
    "    if os.path.exists(opt_model_path):\n",
    "        print(f\"Converted model: {opt_model_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"failed to convert model: {opt_model_path}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec54a26-474c-4285-8a01-edf659391aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: data/bu_savedmodel/assets\n",
      "Saved: data/bu_sleap_model/best_model.h5 -> data/bu_savedmodel\n",
      "Created ModelOptimizer with: data/bu_savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: data/bu_trt_FP32/assets\n",
      "Converted model: data/bu_trt_FP32\n",
      "Created ModelOptimizer with: data/bu_savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: data/bu_trt_FP16/assets\n",
      "Converted model: data/bu_trt_FP16\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: data/centroid_savedmodel/assets\n",
      "Saved: data/centroid_sleap_model/best_model.h5 -> data/centroid_savedmodel\n",
      "Created ModelOptimizer with: data/centroid_savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: data/centroid_trt_FP32/assets\n",
      "Converted model: data/centroid_trt_FP32\n",
      "Created ModelOptimizer with: data/centroid_savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: data/centroid_trt_FP16/assets\n",
      "Converted model: data/centroid_trt_FP16\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: data/td_savedmodel/assets\n",
      "Saved: data/td_sleap_model/best_model.h5 -> data/td_savedmodel\n",
      "Created ModelOptimizer with: data/td_savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: data/td_trt_FP32/assets\n",
      "Converted model: data/td_trt_FP32\n",
      "Created ModelOptimizer with: data/td_savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: data/td_trt_FP16/assets\n",
      "Converted model: data/td_trt_FP16\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    convert_to_savedmodel(model)\n",
    "    \n",
    "    for precision in precisions:\n",
    "        convert_to_trt(model, precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
