{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1eaaa5-1e44-4db1-bd63-5d52ceff21f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: 1/1 available\n",
      "  Device: /physical_device:GPU:0\n",
      "         Available: True\n",
      "        Initalized: False\n",
      "     Memory growth: True\n"
     ]
    }
   ],
   "source": [
    "from models import TopDownInferenceModel, TopDownIDInferenceModel, SingleInstanceInferenceModel\n",
    "import tensorflow as tf\n",
    "from video import read_frames\n",
    "import numpy as np\n",
    "import system\n",
    "from peak_finding import describe_tensors\n",
    "from trtutils import convert_to_trt, OptimizedModel\n",
    "\n",
    "system.disable_preallocation()\n",
    "system.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7662cf7-b417-4251-a9d1-a200eafe7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_of_centroid = \"sleap-data/datasets/wang_4mice_john/labels.full/models.random_split1/centroid.210507_132508.centroid.n=900\"\n",
    "mice_of_td = \"sleap-data/experiments/best_model_replicates/mice_of_td.210509_231024.centered_instance.n=900\"\n",
    "mice_of_clips = \"\"\"\n",
    "sleap-data/datasets/wang_4mice_john/clips/OFTephys-0055-08@14616-18736.mp4\n",
    "sleap-data/datasets/wang_4mice_john/clips/OFTsocialgroup-0000-00@117000-121700.mp4\n",
    "sleap-data/datasets/wang_4mice_john/clips/OFTsocial5mice-0000-00.mp4.old.predictions@3700-6260.mp4\n",
    "\"\"\".strip().split()\n",
    "\n",
    "\n",
    "flies13_centroid = \"sleap-data/datasets/wt_gold.13pt/models.tracking_split2/centroid.fast.210504_182918.centroid.n=1800\"\n",
    "flies13_td = \"sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_fast.210505_012601.centered_instance.n=1800\"\n",
    "flies13_td_id = \"sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_id.fast.v2.210519_111253.multi_class_topdown.n=1800\"\n",
    "flies13_clips = \"\"\"\n",
    "sleap-data/datasets/wt_gold.13pt/clips/single_fly@100000-110000.mp4\n",
    "sleap-data/datasets/wt_gold.13pt/clips/190719_090330_wt_18159206_rig1.2@15000-17560.mp4\n",
    "sleap-data/datasets/wt_gold.13pt/clips/three_flies@8800-11000.mp4\n",
    "sleap-data/datasets/wt_gold.13pt/clips/four_flies@1000-11000.mp4\n",
    "sleap-data/datasets/wt_gold.13pt/clips/eight_flies@180000-200000.mp4\n",
    "\"\"\".strip().split()\n",
    "\n",
    "\n",
    "fly32_single = \"sleap-data/datasets/BermanFlies/models.random_split1/single.fast_unet32.210524_171130.single_instance.n=1350\"\n",
    "fly32_clips = [\"sleap-data/datasets/BermanFlies/clips/072212_163153@10000-13200.mp4\"]\n",
    "\n",
    "\n",
    "gerbils_centroid = \"sleap-data/datasets/nyu-gerbils/cohort1_compressedTalmo_23vids_march_7_to_march_17/models.random_split1.day001/centroid.210504_225945.centroid.n=383\"\n",
    "gerbils_td_id = \"sleap-data/datasets/nyu-gerbils/cohort1_compressedTalmo_23vids_march_7_to_march_17/models.random_split1.day001/td_id.210505_002058.multi_class_topdown.n=383\"\n",
    "gerbils_clips = [\"sleap-data/datasets/nyu-gerbils/clips/2020-3-10_daytime_5mins_compressedTalmo@3200-5760.mp4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf04383a-de6f-4e44-9ee5-24bdb1c7b9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1280, 1024, 1)\n",
      "[<KerasTensor: shape=(None, 640, 512, 1) dtype=float32 (created by layer 'input')>] [<KerasTensor: shape=(None, 320, 256, 1) dtype=float32 (created by layer 'CentroidConfmapsHead_0')>]\n",
      "[<KerasTensor: shape=(None, 352, 352, 1) dtype=float32 (created by layer 'input')>] [<KerasTensor: shape=(None, 88, 88, 11) dtype=float32 (created by layer 'CenteredInstanceConfmapsHead_0')>]\n",
      "WARNING:tensorflow:From /root/miniconda3/envs/trt/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "    instance_peaks: type=ndarray, shape=(80, 11, 2), dtype=float32, device=N/A\n",
      "instance_peak_vals: type=ndarray, shape=(80, 11), dtype=float32, device=N/A\n",
      "       sample_inds: type=ndarray, shape=(80,), dtype=int32, device=N/A\n",
      "INFO:tensorflow:Assets written to: sleap-data/experiments/best_model_replicates/mice_of_td.210509_231024.centered_instance.n=900/savedmodel/assets\n",
      "Created ModelOptimizer with: sleap-data/experiments/best_model_replicates/mice_of_td.210509_231024.centered_instance.n=900/savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_10 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_8 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_9 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_7 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_6 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_5 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_4 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_11 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_17 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_15 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_16 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_14 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_13 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_12 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_18 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: sleap-data/experiments/best_model_replicates/mice_of_td.210509_231024.centered_instance.n=900/trtmodel_FP16/assets\n",
      "Converted model: sleap-data/experiments/best_model_replicates/mice_of_td.210509_231024.centered_instance.n=900/trtmodel_FP16\n",
      "Created ModelOptimizer with: sleap-data/experiments/best_model_replicates/mice_of_td.210509_231024.centered_instance.n=900/savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_9 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_7 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_8 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_6 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_5 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_4 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_10 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_16 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_14 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_15 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_13 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_12 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_11 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_17 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: sleap-data/experiments/best_model_replicates/mice_of_td.210509_231024.centered_instance.n=900/trtmodel_FP32/assets\n",
      "Converted model: sleap-data/experiments/best_model_replicates/mice_of_td.210509_231024.centered_instance.n=900/trtmodel_FP32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_model_path = mice_of_centroid\n",
    "td_model_path = mice_of_td\n",
    "clip_path = mice_of_clips[2]\n",
    "\n",
    "centroid_model = tf.keras.models.load_model(f\"{centroid_model_path}/best_model.h5\", compile=False)\n",
    "td_model = tf.keras.models.load_model(f\"{td_model_path}/best_model.h5\", compile=False)\n",
    "\n",
    "inference_model = TopDownInferenceModel(\n",
    "    centroid_base_model=centroid_model,\n",
    "    centroid_input_size=tuple(centroid_model.inputs[0].shape[1:3]),\n",
    "    centroid_input_scale=0.5,\n",
    "    centroid_output_stride=centroid_model.inputs[0].shape[1] // centroid_model.outputs[0].shape[1],\n",
    "    crop_size=td_model.inputs[0].shape[1],\n",
    "    td_base_model=td_model,\n",
    "    td_output_stride=td_model.inputs[0].shape[1] // td_model.outputs[0].shape[1],\n",
    ")\n",
    "test_imgs = read_frames(clip_path, np.arange(16))\n",
    "print(test_imgs.shape)\n",
    "print(centroid_model.inputs, centroid_model.outputs)\n",
    "print(td_model.inputs, td_model.outputs)\n",
    "\n",
    "preds = inference_model.predict(test_imgs)\n",
    "describe_tensors(preds)\n",
    "\n",
    "inference_model.save(f\"{td_model_path}/savedmodel\", save_format=\"tf\", save_traces=True)\n",
    "convert_to_trt(f\"{td_model_path}/savedmodel\", f\"{td_model_path}/trtmodel\", precision=\"FP16\")\n",
    "convert_to_trt(f\"{td_model_path}/savedmodel\", f\"{td_model_path}/trtmodel\", precision=\"FP32\")\n",
    "\n",
    "# trt_model = OptimizedModel(f\"{td_model_path}/trtmodel_FP16\")\n",
    "# preds = trt_model.predict(test_imgs)\n",
    "# describe_tensors(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dddd1669-b901-49a3-8c1d-fc22ab51ee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sleap-data/datasets/BermanFlies/models.random_split1/single.fast_unet32.210524_171130.single_instance.n=1350/savedmodel/assets\n",
      "Created ModelOptimizer with: sleap-data/datasets/BermanFlies/models.random_split1/single.fast_unet32.210524_171130.single_instance.n=1350/savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_6 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_4 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_5 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_2_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: sleap-data/datasets/BermanFlies/models.random_split1/single.fast_unet32.210524_171130.single_instance.n=1350/trtmodel_FP16/assets\n",
      "Converted model: sleap-data/datasets/BermanFlies/models.random_split1/single.fast_unet32.210524_171130.single_instance.n=1350/trtmodel_FP16\n",
      "Created ModelOptimizer with: sleap-data/datasets/BermanFlies/models.random_split1/single.fast_unet32.210524_171130.single_instance.n=1350/savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_6 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_4 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_5 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_3_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: sleap-data/datasets/BermanFlies/models.random_split1/single.fast_unet32.210524_171130.single_instance.n=1350/trtmodel_FP32/assets\n",
      "Converted model: sleap-data/datasets/BermanFlies/models.random_split1/single.fast_unet32.210524_171130.single_instance.n=1350/trtmodel_FP32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = fly32_single\n",
    "clip_path = fly32_clips[0]\n",
    "\n",
    "model = tf.keras.models.load_model(f\"{model_path}/best_model.h5\", compile=False)\n",
    "\n",
    "inference_model = SingleInstanceInferenceModel(\n",
    "    base_model=model,\n",
    "    output_stride=model.inputs[0].shape[1] // model.outputs[0].shape[1],\n",
    ")\n",
    "test_imgs = read_frames(clip_path, np.arange(4))\n",
    "preds = inference_model.predict(test_imgs)\n",
    "\n",
    "\n",
    "\n",
    "inference_model.save(f\"{model_path}/savedmodel\", save_format=\"tf\", save_traces=True)\n",
    "convert_to_trt(f\"{model_path}/savedmodel\", f\"{model_path}/trtmodel\", precision=\"FP16\")\n",
    "convert_to_trt(f\"{model_path}/savedmodel\", f\"{model_path}/trtmodel\", precision=\"FP32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2bdc46f-834a-4947-aa85-9fd4d27efafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# td_model_path = mice_of_td\n",
    "# clip_path = mice_of_clips[1]\n",
    "# trt_model = OptimizedModel(f\"{td_model_path}/trtmodel_FP16\")\n",
    "\n",
    "\n",
    "# test_imgs = read_frames(clip_path, np.arange(4))\n",
    "# trt_model.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d224753-46e1-49b2-8d3d-e2879e11aaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_fast.210505_012601.centered_instance.n=1800/savedmodel/assets\n",
      "Created ModelOptimizer with: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_fast.210505_012601.centered_instance.n=1800/savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_10 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_8 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_9 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_7 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_6 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_5 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_4 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_11 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_17 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_15 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_16 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_14 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_13 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_12 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_4_18 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_fast.210505_012601.centered_instance.n=1800/trtmodel_FP16/assets\n",
      "Converted model: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_fast.210505_012601.centered_instance.n=1800/trtmodel_FP16\n",
      "Created ModelOptimizer with: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_fast.210505_012601.centered_instance.n=1800/savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_9 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_7 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_8 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_6 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_5 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_4 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_10 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_16 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_14 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_15 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_13 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_12 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_11 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_5_17 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_fast.210505_012601.centered_instance.n=1800/trtmodel_FP32/assets\n",
      "Converted model: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_fast.210505_012601.centered_instance.n=1800/trtmodel_FP32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_model_path = flies13_centroid\n",
    "td_model_path = flies13_td\n",
    "\n",
    "centroid_model = tf.keras.models.load_model(f\"{centroid_model_path}/best_model.h5\", compile=False)\n",
    "td_model = tf.keras.models.load_model(f\"{td_model_path}/best_model.h5\", compile=False)\n",
    "\n",
    "inference_model = TopDownInferenceModel(\n",
    "    centroid_base_model=centroid_model,\n",
    "    centroid_input_size=tuple(centroid_model.inputs[0].shape[1:3]),\n",
    "    centroid_input_scale=0.5,\n",
    "    centroid_output_stride=centroid_model.inputs[0].shape[1] // centroid_model.outputs[0].shape[1],\n",
    "    crop_size=td_model.inputs[0].shape[1],\n",
    "    td_base_model=td_model,\n",
    "    td_output_stride=td_model.inputs[0].shape[1] // td_model.outputs[0].shape[1],\n",
    ")\n",
    "test_imgs = read_frames(flies13_clips[1], np.arange(4))\n",
    "preds = inference_model.predict(test_imgs)\n",
    "inference_model.save(f\"{td_model_path}/savedmodel\", save_format=\"tf\", save_traces=True)\n",
    "convert_to_trt(f\"{td_model_path}/savedmodel\", f\"{td_model_path}/trtmodel\", precision=\"FP16\")\n",
    "convert_to_trt(f\"{td_model_path}/savedmodel\", f\"{td_model_path}/trtmodel\", precision=\"FP32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531956d6-c785-44ae-99a4-5ac46158f36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_id.fast.v2.210519_111253.multi_class_topdown.n=1800/savedmodel/assets\n",
      "Created ModelOptimizer with: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_id.fast.v2.210519_111253.multi_class_topdown.n=1800/savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_10 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_8 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_9 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_7 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_6 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_5 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_4 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_11 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_17 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_15 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_16 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_14 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_13 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_12 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_6_18 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_id.fast.v2.210519_111253.multi_class_topdown.n=1800/trtmodel_FP16/assets\n",
      "Converted model: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_id.fast.v2.210519_111253.multi_class_topdown.n=1800/trtmodel_FP16\n",
      "Created ModelOptimizer with: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_id.fast.v2.210519_111253.multi_class_topdown.n=1800/savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_9 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_7 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_8 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_6 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_5 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_4 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_10 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_16 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_14 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_15 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_13 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_12 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_11 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_7_17 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_id.fast.v2.210519_111253.multi_class_topdown.n=1800/trtmodel_FP32/assets\n",
      "Converted model: sleap-data/datasets/wt_gold.13pt/models.tracking_split2/td_id.fast.v2.210519_111253.multi_class_topdown.n=1800/trtmodel_FP32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_model_path = flies13_centroid\n",
    "td_model_path = flies13_td_id\n",
    "clip_path = flies13_clips[1]\n",
    "\n",
    "centroid_model = tf.keras.models.load_model(f\"{centroid_model_path}/best_model.h5\", compile=False)\n",
    "td_model = tf.keras.models.load_model(f\"{td_model_path}/best_model.h5\", compile=False)\n",
    "\n",
    "inference_model = TopDownIDInferenceModel(\n",
    "    centroid_base_model=centroid_model,\n",
    "    centroid_input_size=tuple(centroid_model.inputs[0].shape[1:3]),\n",
    "    centroid_input_scale=0.5,\n",
    "    centroid_output_stride=centroid_model.inputs[0].shape[1] // centroid_model.outputs[0].shape[1],\n",
    "    crop_size=td_model.inputs[0].shape[1],\n",
    "    td_base_model=td_model,\n",
    "    td_output_stride=td_model.inputs[0].shape[1] // td_model.outputs[0].shape[1],\n",
    ")\n",
    "test_imgs = read_frames(clip_path, np.arange(4))\n",
    "preds = inference_model.predict(test_imgs)\n",
    "inference_model.save(f\"{td_model_path}/savedmodel\", save_format=\"tf\", save_traces=True)\n",
    "convert_to_trt(f\"{td_model_path}/savedmodel\", f\"{td_model_path}/trtmodel\", precision=\"FP16\")\n",
    "convert_to_trt(f\"{td_model_path}/savedmodel\", f\"{td_model_path}/trtmodel\", precision=\"FP32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "215abf72-154b-4275-a36a-ca6da045f4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/miniconda3/envs/trt/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
      "INFO:tensorflow:Assets written to: sleap-data/datasets/nyu-gerbils/cohort1_compressedTalmo_23vids_march_7_to_march_17/models.random_split1.day001/td_id.210505_002058.multi_class_topdown.n=383/savedmodel/assets\n",
      "Created ModelOptimizer with: sleap-data/datasets/nyu-gerbils/cohort1_compressedTalmo_23vids_march_7_to_march_17/models.random_split1.day001/td_id.210505_002058.multi_class_topdown.n=383/savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_11 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_8 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_9 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_10 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_7 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_6 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_5 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_4 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_12 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_19 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_16 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_17 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_18 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_15 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_14 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_13 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_0_20 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: sleap-data/datasets/nyu-gerbils/cohort1_compressedTalmo_23vids_march_7_to_march_17/models.random_split1.day001/td_id.210505_002058.multi_class_topdown.n=383/trtmodel_FP16/assets\n",
      "Converted model: sleap-data/datasets/nyu-gerbils/cohort1_compressedTalmo_23vids_march_7_to_march_17/models.random_split1.day001/td_id.210505_002058.multi_class_topdown.n=383/trtmodel_FP16\n",
      "Created ModelOptimizer with: sleap-data/datasets/nyu-gerbils/cohort1_compressedTalmo_23vids_march_7_to_march_17/models.random_split1.day001/td_id.210505_002058.multi_class_topdown.n=383/savedmodel\n",
      "INFO:tensorflow:Linked TensorRT version: (7, 2, 2)\n",
      "INFO:tensorflow:Loaded TensorRT version: (7, 2, 3)\n",
      "INFO:tensorflow:Loaded TensorRT 7.2.3 and linked TensorFlow against TensorRT 7.2.2. This is supported because TensorRT  minor/patch upgrades are backward compatible\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_10 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_7 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_8 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_9 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_6 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_2 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_0 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_5 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_4 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_3 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_11 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_18 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_15 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_16 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_17 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_14 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_1 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_13 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_12 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Could not find TRTEngineOp_1_19 in TF-TRT cache. This can happen if build() is not called, which means TensorRT engines will be built and cached at runtime.\n",
      "INFO:tensorflow:Assets written to: sleap-data/datasets/nyu-gerbils/cohort1_compressedTalmo_23vids_march_7_to_march_17/models.random_split1.day001/td_id.210505_002058.multi_class_topdown.n=383/trtmodel_FP32/assets\n",
      "Converted model: sleap-data/datasets/nyu-gerbils/cohort1_compressedTalmo_23vids_march_7_to_march_17/models.random_split1.day001/td_id.210505_002058.multi_class_topdown.n=383/trtmodel_FP32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_model_path = gerbils_centroid\n",
    "td_model_path = gerbils_td_id\n",
    "clip_path = gerbils_clips[0]\n",
    "\n",
    "centroid_model = tf.keras.models.load_model(f\"{centroid_model_path}/best_model.h5\", compile=False)\n",
    "td_model = tf.keras.models.load_model(f\"{td_model_path}/best_model.h5\", compile=False)\n",
    "\n",
    "inference_model = TopDownIDInferenceModel(\n",
    "    centroid_base_model=centroid_model,\n",
    "    centroid_input_size=tuple(centroid_model.inputs[0].shape[1:3]),\n",
    "    centroid_input_scale=0.5,\n",
    "    centroid_output_stride=centroid_model.inputs[0].shape[1] // centroid_model.outputs[0].shape[1],\n",
    "    crop_size=td_model.inputs[0].shape[1],\n",
    "    td_base_model=td_model,\n",
    "    td_output_stride=td_model.inputs[0].shape[1] // td_model.outputs[0].shape[1],\n",
    ")\n",
    "test_imgs = read_frames(clip_path, np.arange(4), grayscale=False)\n",
    "preds = inference_model.predict(test_imgs)\n",
    "inference_model.save(f\"{td_model_path}/savedmodel\", save_format=\"tf\", save_traces=True)\n",
    "convert_to_trt(f\"{td_model_path}/savedmodel\", f\"{td_model_path}/trtmodel\", precision=\"FP16\")\n",
    "convert_to_trt(f\"{td_model_path}/savedmodel\", f\"{td_model_path}/trtmodel\", precision=\"FP32\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
